{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17db558e",
   "metadata": {},
   "source": [
    "## Web scraping for bisInfo\n",
    "Example use case: Singapore tenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af5c5507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bee329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "}\n",
    "\n",
    "LIST_URL = \"https://www.bidsinfo.com/country/singapore-tenders\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d49cacaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    \"\"\"Fetch a webpage using Selenium and webdriver-manager for ChromeDriver.\"\"\"\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    from bs4 import BeautifulSoup\n",
    "    import time\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--window-size=1920,1080')\n",
    "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Wait for page to load (adjust if needed)\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "def scrape_tenders(soup):\n",
    "    tenders = []\n",
    "    articles = soup.find_all('article', class_='ee-post')\n",
    "    for idx, article in enumerate(articles, 1):\n",
    "        # Title and detail URL\n",
    "        title_tag = article.find('a', class_='bde-text-link-111-107')\n",
    "        title = \"\"\n",
    "        detail_url = \"\"\n",
    "        if title_tag:\n",
    "            h5 = title_tag.find('h5')\n",
    "            title = h5.get_text(strip=True) if h5 else title_tag.get_text(strip=True)\n",
    "            detail_url = title_tag.get('href', \"\")\n",
    "        # Organization (first icon-list text)\n",
    "        org = \"\"\n",
    "        country = \"\"\n",
    "        publish_date = \"\"\n",
    "        deadline_date = \"\"\n",
    "        icon_texts = article.find_all('span', class_='bde-icon-list__text')\n",
    "        if icon_texts:\n",
    "            org = icon_texts[0].get_text(strip=True) if len(icon_texts) > 0 else \"\"\n",
    "            country = icon_texts[1].get_text(strip=True) if len(icon_texts) > 1 else \"\"\n",
    "            # Publish date is usually the third\n",
    "            publish_date = icon_texts[2].get_text(strip=True) if len(icon_texts) > 2 else \"\"\n",
    "            # Deadline/Closing date is usually the fourth, may have 'Closing Date:' prefix\n",
    "            if len(icon_texts) > 3:\n",
    "                deadline_raw = icon_texts[3].get_text(strip=True)\n",
    "                if 'Closing Date:' in deadline_raw:\n",
    "                    deadline_date = deadline_raw.replace('Closing Date:', '').strip()\n",
    "                else:\n",
    "                    deadline_date = deadline_raw\n",
    "        tenders.append({\n",
    "            \"no\": idx,\n",
    "            \"title\": title,\n",
    "            \"organization\": org,\n",
    "            \"country\": country,\n",
    "            \"publish_date\": publish_date,\n",
    "            \"deadline_date\": deadline_date,\n",
    "            \"detail_url\": detail_url\n",
    "        })\n",
    "    return tenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c035de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"output\", exist_ok=True)\n",
    "    soup = get_page(LIST_URL)\n",
    "    # Save HTML for inspection\n",
    "    with open(\"output/listing_page.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(soup.prettify()))\n",
    "    # Scraped tenders for the first page\n",
    "    tenders = scrape_tenders(soup)\n",
    "    with open(\"output/scrap_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(tenders, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ada8ca",
   "metadata": {},
   "source": [
    "blocked by cloudfare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47175c10",
   "metadata": {},
   "source": [
    "### Another approach to scrap bidsInfo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c34b4fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout waiting for real content: Message: \n",
      "Stacktrace:\n",
      "0   chromedriver                        0x00000001008caecc cxxbridge1$str$ptr + 2941512\n",
      "1   chromedriver                        0x00000001008c2b88 cxxbridge1$str$ptr + 2907908\n",
      "2   chromedriver                        0x00000001003da2b0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74020\n",
      "3   chromedriver                        0x000000010042188c _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 366336\n",
      "4   chromedriver                        0x0000000100462d54 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 633800\n",
      "5   chromedriver                        0x0000000100415ef0 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318820\n",
      "6   chromedriver                        0x000000010088e0c8 cxxbridge1$str$ptr + 2692164\n",
      "7   chromedriver                        0x00000001008918dc cxxbridge1$str$ptr + 2706520\n",
      "8   chromedriver                        0x000000010086e84c cxxbridge1$str$ptr + 2563016\n",
      "9   chromedriver                        0x00000001008921b4 cxxbridge1$str$ptr + 2708784\n",
      "10  chromedriver                        0x00000001008600f4 cxxbridge1$str$ptr + 2503792\n",
      "11  chromedriver                        0x00000001008b1498 cxxbridge1$str$ptr + 2836500\n",
      "12  chromedriver                        0x00000001008b161c cxxbridge1$str$ptr + 2836888\n",
      "13  chromedriver                        0x00000001008c27d8 cxxbridge1$str$ptr + 2906964\n",
      "14  libsystem_pthread.dylib             0x0000000184590c08 _pthread_start + 136\n",
      "15  libsystem_pthread.dylib             0x000000018458bba8 thread_start + 8\n",
      "\n",
      "Scraped 0 tenders.\n"
     ]
    }
   ],
   "source": [
    "# Improved Selenium scraping for Cloudflare-protected pages\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "def get_real_page(url, wait_selector=None, wait_time=15):\n",
    "    \"\"\"\n",
    "    Use Selenium to bypass Cloudflare and wait for real content.\n",
    "    wait_selector: CSS selector for an element that only appears on the real page.\n",
    "    wait_time: Max seconds to wait for the real content.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--window-size=1920,1080')\n",
    "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for Cloudflare to finish and real content to appear\n",
    "    if wait_selector:\n",
    "        try:\n",
    "            WebDriverWait(driver, wait_time).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, wait_selector))\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Timeout waiting for real content: {e}\")\n",
    "    else:\n",
    "        time.sleep(wait_time)\n",
    "\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    return html\n",
    "\n",
    "# Example usage: wait for a real tender article to appear\n",
    "LIST_URL = \"https://www.bidsinfo.com/country/singapore-tenders\"\n",
    "REAL_CONTENT_SELECTOR = \"article.ee-post\"  # Change if needed for your target page\n",
    "\n",
    "html = get_real_page(LIST_URL, wait_selector=REAL_CONTENT_SELECTOR, wait_time=20)\n",
    "\n",
    "# Save the correct HTML for inspection\n",
    "with open(\"output/listing_page.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html)\n",
    "\n",
    "# Parse and extract tenders as before\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "tenders = scrape_tenders(soup)\n",
    "with open(\"output/scrap_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    import json\n",
    "    json.dump(tenders, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Scraped {len(tenders)} tenders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0b4860",
   "metadata": {},
   "source": [
    "Still blocked by Cloudfare"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
